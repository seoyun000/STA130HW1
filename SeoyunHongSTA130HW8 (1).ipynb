{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ac86dc",
   "metadata": {},
   "source": [
    "1. Accuracy\n",
    "Expanded Example: A factory producing light bulbs performs routine checks on 1,000 bulbs, of which 950 are functional (true positives or TNs) and 50 are defective (FPs or FNs). The goal is to assess overall production quality.\n",
    "Detailed Rationale: Accuracy is most appropriate when the costs of false positives (e.g., misidentifying a defective bulb as functional) and false negatives (e.g., discarding a functional bulb) are roughly equal. In such scenarios, you want a single measure of overall correctness, as there isn’t a strong need to differentiate between positive and negative cases.\n",
    "\n",
    "2. Sensitivity\n",
    "Expanded Example: In cervical cancer screenings, sensitivity ensures that most individuals with cancer (true positives) are identified by the test, even if some healthy individuals are mistakenly flagged as having cancer.\n",
    "Detailed Rationale: Sensitivity is especially critical in early-stage disease detection, where missing a true case (false negative) can have severe consequences. For example, undetected cancer can progress and become harder to treat. While false positives (FPs) might result in follow-up tests, they are generally less harmful than false negatives.\n",
    "\n",
    "3. Specificity\n",
    "Expanded Example: A COVID-19 confirmatory PCR test is conducted after a positive rapid antigen test to confirm whether someone truly has the disease.\n",
    "Detailed Rationale: Specificity is crucial here because false positives from the initial screening test could lead to unnecessary isolation or treatments. A highly specific follow-up test ensures that individuals who are truly negative are not falsely identified as positive, reducing unnecessary stress, cost, and potential resource waste.\n",
    "\n",
    "4. Precision\n",
    "Expanded Example: In a spam email filter system, if 100 emails are flagged as spam and only 80 are truly spam, the precision is 80%.\n",
    "Detailed Rationale: Precision is essential in this case because it directly impacts user satisfaction. If precision is low (e.g., many false positives, where important emails are flagged as spam), users might miss critical communications. This is especially relevant in applications where users can’t afford to have false positives, such as business or legal communications.\n",
    "\n",
    "Additional Notes on Real-World Importance\n",
    "Balance Between Sensitivity and Specificity:\n",
    "In many applications (e.g., medical tests), there's often a tradeoff between sensitivity and specificity. For example, an initial cancer screening test might prioritize high sensitivity to \"catch all possible cases,\" followed by a confirmatory test with high specificity to reduce false positives.\n",
    "\n",
    "Accuracy's Pitfall in Imbalanced Data:\n",
    "Accuracy can be misleading in datasets with imbalanced classes. For instance, if only 1% of a population has a disease, predicting \"no disease\" for everyone will yield 99% accuracy but completely fail in identifying positive cases.\n",
    "\n",
    "Precision vs. Recall (Sensitivity):\n",
    "Precision is more useful when false positives are costlier than false negatives. Conversely, sensitivity (recall) is prioritized when missing true positives is more critical. For example, in fraud detection, missing fraudulent transactions (low sensitivity) might be costlier than investigating legitimate transactions flagged as fraudulent (lower precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e0eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1580de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'ab_reduced_noNaN' is the dataframe you are working with\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mget_dummies(ab_reduced_noNaN[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m ab_reduced_noNaN[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Perform the 80/20 split\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'ab_reduced_noNaN' is the dataframe you are working with\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])[\"H\"]\n",
    "X = ab_reduced_noNaN[[\"List Price\"]]\n",
    "\n",
    "# Perform the 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Training data observations: {X_train.shape[0]}\")\n",
    "print(f\"Testing data observations: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6ce5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize and train the decision tree\u001b[39;00m\n\u001b[1;32m      6\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Visualize the decision tree\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize and train the decision tree\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, feature_names=[\"List Price\"], class_names=[\"Paper\", \"Hard\"], filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa2aadc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model (optional)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0f2fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming X_test and y_test are already defined\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y_pred_clf \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m)\n\u001b[1;32m      5\u001b[0m y_pred_clf2 \u001b[38;5;241m=\u001b[39m clf2\u001b[38;5;241m.\u001b[39mpredict(X_test)  \u001b[38;5;66;03m# Replace clf2 with your second trained model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Confusion matrix for clf\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "# Assuming X_test and y_test are already defined\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "y_pred_clf2 = clf2.predict(X_test)  # Replace clf2 with your second trained model\n",
    "# Confusion matrix for clf\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "\n",
    "# Confusion matrix for clf2\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "print(\"Confusion Matrix for clf:\")\n",
    "print(cm_clf)\n",
    "\n",
    "print(\"Confusion Matrix for clf2:\")\n",
    "print(cm_clf2)\n",
    "[[TN, FP],\n",
    " [FN, TP]]\n",
    "# For clf\n",
    "TN_clf, FP_clf, FN_clf, TP_clf = cm_clf.ravel()\n",
    "\n",
    "# For clf2\n",
    "TN_clf2, FP_clf2, FN_clf2, TP_clf2 = cm_clf2.ravel()\n",
    "\n",
    "# For clf\n",
    "sensitivity_clf = TP_clf / (TP_clf + FN_clf)\n",
    "specificity_clf = TN_clf / (TN_clf + FP_clf)\n",
    "accuracy_clf = (TP_clf + TN_clf) / (TP_clf + TN_clf + FP_clf + FN_clf)\n",
    "\n",
    "# For clf2\n",
    "sensitivity_clf2 = TP_clf2 / (TP_clf2 + FN_clf2)\n",
    "specificity_clf2 = TN_clf2 / (TN_clf2 + FP_clf2)\n",
    "accuracy_clf2 = (TP_clf2 + TN_clf2) / (TP_clf2 + TN_clf2 + FP_clf2 + FN_clf2)\n",
    "\n",
    "\n",
    "# Round results to 3 significant digits\n",
    "results = {\n",
    "    \"clf\": {\n",
    "        \"Sensitivity\": np.round(sensitivity_clf, 3),\n",
    "        \"Specificity\": np.round(specificity_clf, 3),\n",
    "        \"Accuracy\": np.round(accuracy_clf, 3),\n",
    "    },\n",
    "    \"clf2\": {\n",
    "        \"Sensitivity\": np.round(sensitivity_clf2, 3),\n",
    "        \"Specificity\": np.round(specificity_clf2, 3),\n",
    "        \"Accuracy\": np.round(accuracy_clf2, 3),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Metrics for clf:\")\n",
    "print(results[\"clf\"])\n",
    "\n",
    "print(\"\\nMetrics for clf2:\")\n",
    "print(results[\"clf2\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f2532",
   "metadata": {},
   "source": [
    "Sensitivity (Recall): Measures how well the model identifies actual positives.\n",
    "\n",
    "Sensitivity\n",
    "=\n",
    "TP /\n",
    "TP\n",
    "+\n",
    "FN\n",
    "Sensitivity= \n",
    "TP+FN/\n",
    "TP\n",
    "​\n",
    " \n",
    "Specificity: Measures how well the model identifies actual negatives.\n",
    "\n",
    "Specificity\n",
    "=\n",
    "TN/\n",
    "TN\n",
    "+\n",
    "FP\n",
    "Specificity= \n",
    "TN+FP/\n",
    "TN\n",
    "​\n",
    " \n",
    "Accuracy: Measures the proportion of correct predictions.\n",
    "\n",
    "Accuracy\n",
    "=\n",
    "TP\n",
    "+\n",
    "TN/\n",
    "TP\n",
    "+\n",
    "TN\n",
    "+\n",
    "FP\n",
    "+\n",
    "FN\n",
    "Accuracy= \n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd07f7",
   "metadata": {},
   "source": [
    "The differences between the two confusion matrices arise due to the features used for training the models. In the first case, only the List Price feature is used to make predictions, which might limit the model's ability to differentiate between \"Paper\" and \"Hard\" books. This could lead to a higher number of false positives or false negatives because the single feature may not capture all the patterns necessary for accurate classification.\n",
    "\n",
    "In the second case, additional features such as NumPages and Thick are included alongside List Price. These features likely provide more information and context, enabling the model to make more accurate predictions by distinguishing between the two classes with greater precision.\n",
    "\n",
    "The confusion matrices for clf and clf2 are better because they are evaluated on a separate testing dataset (ab_reduced_noNaN_test) rather than the training dataset (ab_reduced_noNaN_train). Evaluating on the training dataset can lead to overestimated performance, as the model has already \"seen\" the data during training, potentially resulting in overfitting. Testing on a separate dataset provides a more realistic measure of how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e528ad",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673ff691-f03c-8011-be88-998ff377f2a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
